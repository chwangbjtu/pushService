spider手册1 出版本cd Spider/X.X.X/src; tar zcvf spider.tgz *--X.X.X指版本号说明：如果在windows上打包，在src目录中选择所有文件，右键选择WinRAR压缩为zip包（Linux系统中一般有unzip，而没有RAR的解压缩程序）2 部署2.1 依赖：MySQL>=5.6.0, python>=2.7.0, 依赖MySQLDb的python模块（python模块可以用easy_install安装，或者直接下载安装包执行python setup.py install）2.1.1 MySQL安装MySQL安装以5.6.17版本为例：tar zxvf mysql-5.6.17.tar.gzcd cd mysql-5.6.17mkdir bldcd bldcmake .. -DBUILD_CONFIG=mysql_release make说明：1）安装cmakeyum install cmake2) 如提示依赖其他包请按照提示安装2.2 安装目录安装目录默认为/home/spider/将build包上传到服务器的/home/spider/，执行解压缩操作。2.3 配置2.3.1 cloud_taskmgr配置文件为etc/config.ini[server]#service address and portlocal_port = 6600                # cloud_taskmgr监听的端口#maze center address and portmaze_addr = X.X.X.X              # maze地址，必须为云转码可以访问的公网地址maze_port = 6809                 # maze监听的端口 #cloud server address and portcloud_addr = centaurus.funshion.com   # 云转码服务地址，线上不用修改cloud_port = 6531                # 云转码服务端口[log]#log file directoryfile_dir= /media2/log/taskmgr      # cloud_taskmgrlog输出目录#whether to print log on consoleconsole_output= 0#log output level control (critical/error/warning/info/debug/all/level= all[workenv]#restore filefile_dir = /media2/db/taskmgr2     # cloud_taskmgr任务本地持久化任务目录，用于重启后重新提交任务 - maze已经可以重新提交转码任务file_name = tasklist.data#time interval for asking for task (seconds)timeout = 1                        # cloud_taskmgr任务调度间隔#max task count concurrencemax_task_count = 20                # cloud_taskmgr一次向maze申请的最大任务数，和timeout一起控制cloud_taskmgr申请任务的速度2.3.2 maze_service配置文件etc.py#!/usr/bin/python# -*- coding:utf-8 -*-host = 'localhost'       # 数据库配置user = 'root'passwd = '*****'db = 'ugc'port = 3306MAZE_IP = 'X.X.X.X'SERVICE_PORT = 6809DBPATH = "/media2/log/maze2/"   # db目录DATABASENAME = DBPATH + "status_report.db"FORWARDS_TASK_DB = DBPATH + "forwardsdoor.db"LOGPATH = "/media2/log/maze2/"  # log目录MAX_LOGFILE_SIZE = 50 #M...macros_ip      = 'macross.funshion.com:27777'            # macross配置，测试时根据测试环境修改为对应地址和端口，线上不用修改macros_method  = '/api/?cmd=report_video&cli=ugc&user_name=ugc'CLOUD_SERVICE_IP       = 'centaurus.funshion.com'        # 线上不用修改        CLOUD_SERVICE_PORT     = 6531                            #cloud service port.CLOUD_SERVICE_ADD_TASK = "/add_task"...URGENT_USER = [ 1 ]TRANSCODE_FREE = 1                                       # 1 表示只选择一种码率，目前为standard, 如果视频本身码率太低，则只会转码出smoothMASAIC_FREE = 1                                          # 1 表示不需要打masaicLOGO_FREE = 1                                            # 暂不起作用2.3.3 spider web(ugc)配置文件ugc/settings.py#数据库配置DATABASES = {    'default': {        'ENGINE': 'django.db.backends.mysql',   # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.        'NAME': 'ugc',                          # Or path to database file if using sqlite3.        'USER': 'root',                         # Not used with sqlite3.        'PASSWORD': '******',                   # Not used with sqlite3.        'HOST': 'localhost',                    # Set to empty string for localhost. Not used with sqlite3.        'PORT': '3306',                         # Set to empty string for default. Not used with sqlite3.        'OPTIONS': { 'init_command': 'SET storage_engine=INNODB;' }, #存储引擎为InnoDB，支持事务和行级锁    }}...#upload, 上传文件用的目录FILE_UPLOAD_MAX_MEMORY_SIZE = 0FILE_UPLOAD_TEMP_DIR = "/media3/upload_files2/"MAZE_HOST="X.X.X.X"     # maze的公网ipMAZE_PORT="6809"CLOUD_HOST="centaurus.funshion.com" # 云转码的ip地址CLOUD_PORT="6531" 2.4 数据库初始化2.4.1 创建数据库mysql -u root -p进入mysql后执行CREATE DATABASE `ugc` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;2.4.2 spider web(ugc)执行python manage.py syncdb过程中根据提示创建管理员账号2.4.3 其他数据表和存储过程初始化进入install目录中cd installmysql -u root -p ugc < ugc-table-init.sqlmysql -u root -p ugc < ugc-stored-procedure.sql2.5 启动2.5.1 cloud_taskmgrnohup python cloud_taskmgr.py > /dev/null 2>&1 &2.5.2 maze_servicenohup python maze_server.py &2.5.3 spider web(ugc)nohup python manage.py runserver 0.0.0.0:7777 > spider.log &2.6 调试请按照配置查看各个服务的logspider web如果异常可以查看spider.log2.7 监测 - 功能待补充各服务应该使用crontab脚本进行监测，如发现服务crash，应及时拉起服务